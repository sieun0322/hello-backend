# 분산 ID 생성기 성능 분석 보고서

**작성자**: Performance Engineer
**작성일**: 2026-02-08
**버전**: 1.0

---

## 목차
1. [처리량(Throughput) 분석](#1-처리량throughput-분석)
2. [지연시간(Latency) 분석](#2-지연시간latency-분석)
3. [수평 확장 전략](#3-수평-확장horizontal-scaling-전략)
4. [사전 할당 및 배치 생성](#4-사전-할당pre-allocation--배치-생성)
5. [DB 인덱스 성능 영향](#5-db-인덱스-성능-영향)
6. [리소스 사용량 비교](#6-리소스-사용량-비교)
7. [핫스팟 방지 전략](#7-핫스팟-방지-전략)
8. [벤치마크 전략](#8-벤치마크-전략)
9. [종합 권장사항](#9-종합-권장사항)

---

## 1. 처리량(Throughput) 분석

### 1.1 이론적 초당 생성 상한 (단일 노드 기준)

| 방식 | 이론적 상한 (IDs/sec/node) | 병목 요인 | 비고 |
|------|---------------------------|-----------|------|
| **Snowflake (64-bit)** | 4,096,000 | 시퀀스 12bit = 4096/ms | 밀리초당 4,096개, 초당 ~409만 |
| **Sonyflake** | 256,000 | 시퀀스 8bit = 256/10ms | 10ms 단위, 초당 ~25.6만 |
| **Baidu UidGenerator** | 8,192,000 | 시퀀스 13bit, RingBuffer 사전할당 | 초당 ~819만 (캐시 모드) |
| **UUID v4 (랜덤)** | 10,000,000+ | CPU (CSPRNG) | OS 엔트로피 풀 의존 |
| **UUID v7 (시간+랜덤)** | 8,000,000+ | CPU + 시간 함수 | 단조 증가 보장 시 시퀀스 제한 |
| **ULID** | 1,000,000~5,000,000 | 밀리초 내 단조 증가 보장 | 구현체에 따라 차이 큼 |
| **Redis INCR** | 100,000~200,000 | 네트워크 RTT | 파이프라인 시 ~50만 |
| **DB Auto-Increment** | 20,000~80,000 | 디스크 I/O + 잠금 | WAL fsync 정책에 따라 차이 |
| **ZooKeeper znode** | 10,000~20,000 | 합의 프로토콜 (ZAB) | 쓰기 지배적이므로 느림 |
| **etcd** | 10,000~30,000 | Raft 합의 | 쓰기 처리량 제한적 |

### 1.2 Snowflake 처리량 상세 분석

```
Snowflake 64-bit 구조:
┌─────────┬──────────────────────┬────────────┬──────────────┐
│ 부호(1) │  타임스탬프(41bit)    │ 노드ID(10) │ 시퀀스(12)   │
└─────────┴──────────────────────┴────────────┴──────────────┘

이론적 상한 계산:
- 시퀀스: 12bit → 2^12 = 4,096 IDs/ms
- 초당: 4,096 × 1,000 = 4,096,000 IDs/sec/node
- 노드 수: 10bit → 최대 1,024 노드
- 클러스터 이론 상한: 4,096,000 × 1,024 ≈ 41.9억 IDs/sec

실측 성능 (참고치):
- 단일 스레드: ~2,000,000 IDs/sec (메모리/캐시 효과)
- 멀티 스레드 (lock contention): ~800,000~1,500,000 IDs/sec
- CAS 기반 lock-free: ~3,000,000 IDs/sec
```

### 1.3 UUID v4 vs UUID v7 처리량 비교

```
UUID v4 (122-bit 랜덤):
- Linux /dev/urandom 처리량: ~600 MB/s
- UUID 1개 = 16 bytes → 이론 상한: ~37,500,000 IDs/sec
- 실측 (Java UUID.randomUUID()): ~3,000,000~5,000,000 IDs/sec
  (ThreadLocalRandom이 아닌 SecureRandom 사용으로 인한 병목)
- 실측 (Go uuid.NewV4): ~8,000,000~12,000,000 IDs/sec

UUID v7 (48-bit timestamp + 74-bit random):
- 시간 함수 호출 오버헤드 추가
- 단조 증가(monotonic) 보장 시 시퀀스 카운터 필요
- 실측: UUID v4 대비 ~70-90% 수준
- 장점: 정렬 가능하므로 DB 인덱스 성능에서 우위
```

### 1.4 ULID 처리량 분석

```
ULID 구조 (128-bit):
┌──────────────────┬──────────────────────────────┐
│ 타임스탬프(48bit) │      랜덤(80bit)              │
│   6 bytes         │      10 bytes                │
└──────────────────┴──────────────────────────────┘

- 밀리초 정밀도, 같은 ms 내 단조 증가 필요
- 80-bit 랜덤 → 같은 ms 내 2^80 개 가능 (사실상 무한)
- 단, 단조 증가 보장 시 이전 랜덤 값에 +1 → 오버플로 주의
- 실측: ~2,000,000~5,000,000 IDs/sec (구현체 의존)
```

---

## 2. 지연시간(Latency) 분석

### 2.1 생성 방식별 지연시간 프로파일

| 방식 | P50 | P99 | P999 | 네트워크 호출 |
|------|-----|-----|------|--------------|
| **Snowflake (로컬)** | 0.2~0.5 μs | 1~3 μs | 5~15 μs | 없음 |
| **UUID v4** | 0.1~0.3 μs | 0.5~2 μs | 3~10 μs | 없음 |
| **UUID v7** | 0.2~0.5 μs | 1~3 μs | 5~15 μs | 없음 |
| **ULID** | 0.2~0.5 μs | 1~3 μs | 5~10 μs | 없음 |
| **Redis INCR** | 0.3~1 ms | 2~5 ms | 10~50 ms | 1 RTT |
| **Redis (파이프라인)** | 0.1~0.5 ms | 1~3 ms | 5~20 ms | 1 RTT/배치 |
| **DB Sequence** | 0.5~2 ms | 5~15 ms | 20~100 ms | 1 RTT + I/O |
| **ZooKeeper** | 2~5 ms | 10~30 ms | 50~200 ms | 합의 필요 |
| **etcd** | 1~5 ms | 10~30 ms | 50~150 ms | Raft 합의 |

### 2.2 지연시간 분포 특성 분석

```
로컬 생성 (Snowflake/UUID/ULID):
┌─────────────────────────────────────────────────┐
│ P50        P90       P99       P999              │
│ ├──────────┤         │         │                 │
│ 0.2μs    0.5μs     2μs      10μs               │
│                                                  │
│ 특징: 매우 좁은 분포, GC/컨텍스트 스위칭이 테일 원인  │
└─────────────────────────────────────────────────┘

네트워크 기반 (Redis/DB):
┌─────────────────────────────────────────────────┐
│ P50        P90       P99       P999              │
│ ├──────────────────────┤       │                 │
│ 0.5ms    1.5ms      5ms     50ms               │
│                                                  │
│ 특징: 넓은 분포, 네트워크 지터/큐잉이 테일 원인       │
│ P99/P50 비율: ~10x (로컬은 ~5x)                  │
└─────────────────────────────────────────────────┘
```

### 2.3 테일 레이턴시 원인 분석

| 퍼센타일 | 로컬 생성 원인 | 네트워크 생성 원인 |
|----------|---------------|-------------------|
| P99 | GC pause, CPU 캐시 미스 | 네트워크 지터, TCP 재전송 |
| P999 | OS 스케줄러 지연, 페이지 폴트 | 서버 큐잉, 연결 풀 고갈 |
| P9999 | NUMA 간 메모리 접근 | 서버 GC, 장애 복구 |

### 2.4 밀리초 해상도 vs 마이크로초 해상도

```
시간 함수 호출 비용:
- System.currentTimeMillis() (Java): ~25ns
- clock_gettime(CLOCK_REALTIME): ~20ns
- time.Now() (Go): ~30ns
- std::chrono::system_clock::now() (C++): ~20ns

→ 전체 ID 생성 비용에서 시간 함수는 ~5-15% 차지
→ 병목이 아님. 진짜 병목은 동기화(lock/CAS)
```

---

## 3. 수평 확장(Horizontal Scaling) 전략

### 3.1 확장성 모델 비교

```
처리량 = f(노드 수) 관계:

Snowflake:     T(n) = T(1) × n           [완전 선형, 각 노드 독립]
UUID v4:       T(n) = T(1) × n           [완전 선형, 조정 불필요]
UUID v7:       T(n) = T(1) × n           [완전 선형, 조정 불필요]
Redis INCR:    T(n) = T(1) × n × 0.85    [샤딩 시 준선형]
DB Sequence:   T(n) = T(1) × n × 0.7     [복제 오버헤드]
ZooKeeper:     T(n) = T(1) × (1/log(n))  [합의 오버헤드로 역행]
```

### 3.2 방식별 확장 한계

| 방식 | 최대 노드 수 | 확장 한계 원인 | 클러스터 최대 처리량 |
|------|-------------|---------------|-------------------|
| **Snowflake** | 1,024 (10bit) | 노드 ID 비트 수 | ~41.9억/sec |
| **Snowflake 변형** | 4,096 (12bit) | 시퀀스 비트 감소 트레이드오프 | ~10.2억/sec |
| **UUID v4/v7** | 무제한 | 충돌 확률만 고려 | 사실상 무한 |
| **ULID** | 무제한 | 충돌 확률만 고려 | 사실상 무한 |
| **Redis** | ~100 (클러스터) | 네트워크, 관리 복잡도 | ~2,000만/sec |
| **DB Sequence** | ~16 (복제) | 복제 지연, 잠금 | ~100만/sec |

### 3.3 Snowflake 비트 할당 트레이드오프

```
총 63 비트 (부호 제외) 분배 전략:

전략 A: 높은 처리량 (기본)
  타임스탬프(41) + 노드(10) + 시퀀스(12)
  → 1,024 노드 × 4,096/ms = 초당 41.9억
  → 수명: ~69.7년

전략 B: 대규모 클러스터
  타임스탬프(41) + 노드(13) + 시퀀스(9)
  → 8,192 노드 × 512/ms = 초당 41.9억 (총량 동일)
  → 노드당 처리량 감소, 노드 수 증가

전략 C: 초장기 운영
  타임스탬프(42) + 노드(10) + 시퀀스(11)
  → 1,024 노드 × 2,048/ms = 초당 20.9억
  → 수명: ~139.4년

전략 D: 마이크로서비스용 (128-bit 확장)
  타임스탬프(48) + 노드(16) + 시퀀스(16) + 랜덤(48)
  → 65,536 노드 × 65,536/ms = 초당 4조+
  → 128-bit로 확장하면 모든 제약 해소
```

### 3.4 노드 ID 할당 전략

```
1. 정적 설정 (Static Configuration)
   - 배포 시 환경 변수/설정 파일로 지정
   - 장점: 단순, 외부 의존성 없음
   - 단점: 수동 관리, 재사용 위험
   - 적합: 고정 노드 수의 온프레미스 환경

2. ZooKeeper/etcd 기반 (Coordination)
   - 시작 시 임시 노드(ephemeral node) 생성
   - 장점: 자동 할당/회수, 충돌 방지
   - 단점: 외부 의존성, 시작 지연 (~100ms)
   - 적합: 동적 확장 필요한 클라우드 환경

3. DB 시퀀스 기반
   - 시작 시 DB에서 노드 ID 할당
   - 장점: 기존 인프라 활용
   - 단점: DB 의존성, 장애 전파 위험
   - 적합: 이미 DB 중심 아키텍처인 경우

4. IP/MAC 기반 해싱
   - hash(IP + port) % max_node_count
   - 장점: 외부 의존성 없음, 자동
   - 단점: 해시 충돌 가능, 검증 필요
   - 적합: 빠른 프로토타이핑
```

---

## 4. 사전 할당(Pre-allocation) / 배치 생성

### 4.1 사전 할당 전략

```
기본 패턴: ID 범위를 미리 할당받아 로컬에서 소비

중앙 서버                        애플리케이션 노드
┌─────────┐   범위 요청 (1회)    ┌─────────────────┐
│         │ ←──────────────────── │                 │
│ 다음 범위│                      │ 로컬 카운터      │
│ 관리    │ ──────────────────→  │ [1000..1999]    │
│         │   범위 할당 (1회)    │                 │
└─────────┘                      │ ID 생성 (999회)  │
                                 │ 네트워크 없이     │
                                 └─────────────────┘

네트워크 호출 감소율 = 1 - (1/배치크기)
배치크기 1,000 → 네트워크 호출 99.9% 감소
```

### 4.2 배치 크기 최적화

| 배치 크기 | 네트워크 호출 감소 | 장애 시 ID 낭비 | 할당 간격 (10K IDs/sec) | 권장 시나리오 |
|-----------|-------------------|----------------|----------------------|-------------|
| 100 | 99% | 최대 99개 | 10ms | 낮은 트래픽, 낮은 낭비 |
| 1,000 | 99.9% | 최대 999개 | 100ms | **일반적 권장** |
| 10,000 | 99.99% | 최대 9,999개 | 1sec | 고처리량 서비스 |
| 100,000 | 99.999% | 최대 99,999개 | 10sec | 초고처리량, ID 연속성 불필요 |

### 4.3 Segment 방식 (Leaf-Segment, 미단 Leaf 방식)

```
미단(Meituan) Leaf-Segment 구조:

DB 테이블:
┌──────────┬───────────┬──────┬─────────────────┐
│ biz_tag  │ max_id    │ step │ update_time     │
├──────────┼───────────┼──────┼─────────────────┤
│ order    │ 2000000   │ 1000 │ 2026-02-08 ...  │
│ user     │ 500000    │ 500  │ 2026-02-08 ...  │
└──────────┴───────────┴──────┴─────────────────┘

동작:
1. 서비스 시작 시 [max_id+1, max_id+step] 범위 할당
2. DB UPDATE: max_id = max_id + step
3. 로컬에서 범위 내 ID 순차 발급
4. 70% 소진 시 다음 범위 비동기 프리페치 (이중 버퍼)

이중 버퍼(Double Buffer) 최적화:
┌──────────────┐  ┌──────────────┐
│ 현재 세그먼트  │  │ 다음 세그먼트  │
│ [1001..2000] │  │ [2001..3000] │
│ 소비 중 ──→  │  │ 대기 중       │
└──────────────┘  └──────────────┘
  70% 소진 시 ──→ 비동기로 다음 세그먼트 프리페치

효과:
- DB 장애 시에도 현재+다음 세그먼트 소진까지 서비스 가능
- step=1000, 10K IDs/sec → DB 장애 시 ~200ms 여유
- step=100000 → DB 장애 시 ~20sec 여유
```

### 4.4 적응형 배치 크기 (Adaptive Step)

```
트래픽에 따라 배치 크기 자동 조정:

알고리즘:
  if 이전_세그먼트_소진_시간 < T_min (15분):
      next_step = min(current_step × 2, MAX_STEP)
  elif 이전_세그먼트_소진_시간 > T_max (120분):
      next_step = max(current_step / 2, MIN_STEP)
  else:
      next_step = current_step

장점:
- 트래픽 급증 시 DB 부하 자동 감소
- 트래픽 감소 시 ID 낭비 자동 감소
- 수동 설정 불필요
```

---

## 5. DB 인덱스 성능 영향

### 5.1 B-Tree 삽입 패턴: 순차 키 vs 랜덤 키

```
순차 키 (Auto-Increment, Snowflake, ULID):
                    [50,100,150]
                   /      |      \
         [10,20,30,40] [60,70,80,90] [110,120,130,140] → [151,152,153...]
                                                           항상 끝에 추가
삽입 패턴:
- 새 키는 항상 가장 오른쪽 리프에 삽입
- 페이지 분할(page split): 오른쪽으로만 발생 → "추가 전용(append-only)"
- 버퍼 풀 히트율: 높음 (활성 페이지가 1-2개)
- 쓰기 증폭(write amplification): 낮음


랜덤 키 (UUID v4):
                    [50,100,150]
                   /      |      \
         [10,20,30,40] [60,70,80,90] [110,120,130,140]
              ↑              ↑              ↑
          랜덤 삽입       랜덤 삽입       랜덤 삽입

삽입 패턴:
- 새 키가 임의의 리프에 삽입
- 페이지 분할: 전체에 걸쳐 빈번 → "50/50 분할"
- 버퍼 풀 히트율: 낮음 (모든 페이지가 활성)
- 쓰기 증폭(write amplification): 높음
```

### 5.2 정량적 성능 비교

| 지표 | 순차 키 (Snowflake) | 랜덤 키 (UUID v4) | 차이 |
|------|--------------------|--------------------|------|
| **INSERT 처리량** | 40,000~80,000 TPS | 10,000~25,000 TPS | 2~4x 차이 |
| **페이지 분할 빈도** | 낮음 (우측만) | 높음 (전체) | ~10x 차이 |
| **버퍼 풀 히트율** | 95~99% | 50~80% | 인덱스 크기 의존 |
| **디스크 I/O (쓰기)** | 순차 I/O 위주 | 랜덤 I/O 위주 | ~5x IOPS 차이 |
| **인덱스 크기** | 컴팩트 (~70% fill) | 팽창 (~50% fill) | ~40% 크기 증가 |
| **디스크 읽기 (범위 쿼리)** | 연속 페이지 | 분산 페이지 | ~3x 차이 |

### 5.3 데이터 크기별 인덱스 성능 영향

```
인덱스가 메모리에 적합할 때 (데이터 < 버퍼 풀):
  순차 vs 랜덤 차이: 1.2~1.5x (미미)
  → 모든 페이지가 메모리에 있으므로 I/O 패턴 무관

인덱스가 메모리를 초과할 때 (데이터 >> 버퍼 풀):
  순차 vs 랜덤 차이: 3~10x (극적)
  → 랜덤 키: 캐시 미스 → 디스크 랜덤 I/O 폭증
  → 순차 키: 핫 페이지만 메모리에 유지, I/O 최소

실측 예시 (MySQL InnoDB, 16GB 버퍼 풀):
- 데이터 10GB: 순차 45K TPS vs 랜덤 38K TPS (1.2x)
- 데이터 50GB: 순차 42K TPS vs 랜덤 18K TPS (2.3x)
- 데이터 200GB: 순차 38K TPS vs 랜덤 5K TPS (7.6x)
```

### 5.4 UUID v7/ULID의 B-Tree 이점

```
UUID v7 (시간 정렬):
  - 대부분의 INSERT가 최근 시간대 리프 노드에 집중
  - UUID v4 대비 INSERT 성능: 2~5x 향상
  - UUID v4 대비 인덱스 크기: ~20% 감소 (더 나은 fill factor)
  - 시간 범위 쿼리 효율적 (WHERE created_at > ?)

  주의: 완벽한 순차가 아님 (같은 ms 내 랜덤 부분 존재)
  → Auto-Increment보다는 약간 더 많은 페이지 분할
  → 하지만 UUID v4보다는 압도적으로 적음
```

### 5.5 PostgreSQL vs MySQL 인덱스 특성

```
PostgreSQL (Heap + B-Tree 인덱스):
- HOT(Heap-Only Tuple) 업데이트로 인덱스 유지 비용 감소
- BRIN 인덱스: 순차 키에 대해 초경량 인덱스 가능
  → Snowflake/ULID 사용 시 BRIN으로 시간 범위 쿼리 가능
  → 인덱스 크기: B-Tree의 ~1/100
- fillfactor 튜닝: 랜덤 키 시 80→50으로 낮춰 페이지 분할 감소

MySQL InnoDB (클러스터형 인덱스):
- PK가 곧 데이터 정렬 순서 → 순차 키의 이점이 더 극대화
- 랜덤 PK 시 데이터 행 자체의 재배치 발생 → 추가 I/O
- change buffer: 보조 인덱스의 랜덤 삽입을 버퍼링
  → 비고유 보조 인덱스에만 적용, PK에는 미적용
```

---

## 6. 리소스 사용량 비교

### 6.1 ID 당 리소스 소비

| 방식 | ID 크기 | 메모리/ID | CPU/ID | 네트워크/ID |
|------|---------|-----------|--------|------------|
| **Snowflake** | 8 bytes | ~0 (레지스터) | ~50ns | 0 |
| **UUID v4** | 16 bytes | ~0 | ~100ns (CSPRNG) | 0 |
| **UUID v7** | 16 bytes | ~0 | ~80ns | 0 |
| **ULID** | 16 bytes | ~0 | ~80ns | 0 |
| **Redis INCR** | 8 bytes | ~0 (클라이언트) | ~10ns (클라이언트) | ~100 bytes |
| **DB Sequence** | 8 bytes | ~0 | ~10ns | ~200 bytes |

### 6.2 상주 메모리 사용량 (서비스 인프라 포함)

| 방식 | 서비스 메모리 | 외부 의존성 메모리 | 총 메모리 |
|------|-------------|-------------------|----------|
| **Snowflake (내장)** | ~1 KB (상태) | 0 | ~1 KB |
| **UUID (내장)** | ~0 | 0 | ~0 |
| **전용 ID 서비스** | ~50~200 MB (JVM) | 0 | ~200 MB |
| **Redis 기반** | ~10 MB (클라이언트) | ~50 MB (Redis) | ~60 MB |
| **Segment (DB)** | ~10 MB (버퍼) | ~500 MB (DB) | ~510 MB |
| **ZooKeeper 기반** | ~20 MB (클라이언트) | ~500 MB (ZK 앙상블) | ~520 MB |

### 6.3 CPU 프로파일

```
Snowflake ID 생성 CPU 분해:
┌────────────────────────────────┐
│ clock_gettime()     : 20ns (25%)│
│ 시퀀스 CAS 연산     : 30ns (38%)│
│ 비트 시프트/OR 조합  : 15ns (19%)│
│ 함수 호출 오버헤드    : 15ns (18%)│
│ 총계                : 80ns      │
└────────────────────────────────┘

UUID v4 생성 CPU 분해:
┌────────────────────────────────┐
│ CSPRNG 호출         : 70ns (64%)│ ← 병목
│ 버전/변형 비트 설정   : 10ns  (9%)│
│ 문자열 포매팅(선택)  : 30ns (27%)│
│ 총계                : 110ns     │
└────────────────────────────────┘
→ UUID를 바이너리로 저장하면 문자열 포매팅 생략 가능

네트워크 기반 (Redis INCR) CPU 분해:
┌────────────────────────────────┐
│ 직렬화               : 50ns  (5%)│
│ 시스템 호출 (send)    : 200ns(20%)│
│ 네트워크 대기         : 500μs(~99%)│ ← 압도적 병목
│ 역직렬화             : 50ns  (5%)│
│ 총계                 : ~501μs   │
└────────────────────────────────┘
→ CPU보다 I/O wait가 지배적
```

### 6.4 네트워크 대역폭

```
초당 100만 ID 생성 시 네트워크 사용량:

로컬 생성: 0 bytes/sec

Redis INCR (매번 호출):
  요청: ~50 bytes × 1,000,000 = ~47.7 MB/sec
  응답: ~20 bytes × 1,000,000 = ~19.1 MB/sec
  총: ~66.8 MB/sec → 비현실적

Redis + 배치 (1000개씩):
  요청: ~50 bytes × 1,000 = ~48.8 KB/sec
  응답: ~20 bytes × 1,000 = ~19.5 KB/sec
  총: ~68.3 KB/sec → 무시 가능

Segment 방식 (step=1000):
  요청: ~200 bytes × 1,000 = ~195 KB/sec
  응답: ~100 bytes × 1,000 = ~97.7 KB/sec
  총: ~293 KB/sec → 무시 가능
```

---

## 7. 핫스팟 방지 전략

### 7.1 시간 기반 ID의 핫스팟 문제

```
문제: 시간 순서 ID → 최신 데이터가 같은 파티션/샤드에 집중

예시 - 시간 기반 샤딩:
  Shard 1: [00:00~06:00 데이터]  ← 과거, 읽기만
  Shard 2: [06:00~12:00 데이터]  ← 과거, 읽기만
  Shard 3: [12:00~18:00 데이터]  ← 과거, 읽기만
  Shard 4: [18:00~24:00 데이터]  ← 현재, 모든 쓰기 집중! ★ 핫스팟

Write Amplification:
  - 하나의 샤드에 모든 쓰기 집중
  - 해당 샤드의 B-Tree 리프 노드 경합
  - WAL 쓰기, 버퍼 플러시 집중
  - 복제 지연 증가
```

### 7.2 해결 전략

#### 전략 1: Shard-Friendly ID 설계

```
ID 구조에 샤드 키를 명시적으로 포함:

┌──────────┬──────────────────┬────────────┬──────────┐
│ 샤드(8)  │  타임스탬프(40)   │ 노드ID(8)  │ 시퀀스(8) │
└──────────┴──────────────────┴────────────┴──────────┘

샤드 키 결정 방법:
  shard_id = hash(user_id) % num_shards  (비즈니스 키 기반)
  또는
  shard_id = random(0, num_shards-1)     (균등 분산)

효과: 쓰기가 모든 샤드에 균등 분산
단점: ID만으로 시간 정렬 불가 (샤드 내에서만 정렬)
```

#### 전략 2: 비트 인터리빙 (Bit Interleaving)

```
원래 Snowflake ID:
  TTTTTTTTTTTTTTTT TTTTTTTTTTTT NNNNNNNNNN SSSSSSSSSSSS
  (시간이 상위 비트 → 같은 시간대 ID가 인접)

비트 뒤섞기:
  원본: timestamp=0x1A2B3C → 이진: 000110100010101100111100
  뒤섞기 후:                       010001011010001011001110

  XOR 방식: ID = original_id ^ (original_id >> 16)
  → 시간적으로 인접한 ID가 다른 범위로 분산
  → 복원 가능: original_id = shuffled ^ (shuffled >> 16)

효과: 쓰기 분산, 시간 정렬은 복원 후 가능
주의: 범위 쿼리 최적화 상실
```

#### 전략 3: 접두사 솔팅 (Prefix Salting)

```
ID = salt + timestamp + sequence
salt = hash(entity_id) % NUM_SALT_BUCKETS (예: 256)

예시:
  원래: 2026020812345_001 → Shard X (시간 기반)
  솔팅: 0x7F_2026020812345_001 → Shard hash(0x7F) (분산)

HBase/Bigtable에서 특히 효과적:
  - Row key 접두사로 리전 결정
  - 솔트 없이: 모든 쓰기가 한 리전에 집중
  - 솔트 적용: 256개 리전에 균등 분산

trade-off:
  + 쓰기 핫스팟 제거
  - 전체 스캔 시 256개 범위 병합 필요
  - 단일 엔터티의 시간순 조회는 여전히 효율적
```

#### 전략 4: 역 타임스탬프 (Reverse Timestamp)

```
ID = (MAX_TIMESTAMP - current_timestamp) + sequence

효과:
  - 최신 데이터의 ID가 작은 값
  - 최신 조회(최근 N건) 시 정방향 스캔으로 가능
  - 쓰기 위치가 인덱스 시작 부분으로 이동

적합: 최신 데이터 조회가 빈번한 시계열 서비스
```

#### 전략 5: 복합 전략 (Instagram 방식)

```
Instagram의 ID 구조:
  - 41 bits: 밀리초 타임스탬프 (커스텀 에포크)
  - 13 bits: 논리적 샤드 ID (= user_id % 8192)
  - 10 bits: 시퀀스 (각 샤드 내 PG 시퀀스)

핵심 설계:
  1. 샤드 ID가 user_id에 기반 → 같은 유저의 데이터는 같은 샤드
  2. 다른 유저의 데이터는 다른 샤드로 분산
  3. 시간 순서 유지 → 피드/타임라인 정렬 가능
  4. 쓰기 핫스팟: 유저 수가 충분하면 자연 분산
```

### 7.3 핫스팟 정량 분석

```
가정: 1,000 TPS 쓰기, 16 샤드

순차 ID (시간 기반 파티셔닝):
  핫 샤드: ~1,000 TPS (100%)
  나머지 15 샤드: ~0 TPS
  불균형 비율: 16:1

해시 기반 파티셔닝 + 순차 ID:
  각 샤드: ~62.5 TPS (±10%)
  불균형 비율: ~1.2:1

Shard-Friendly ID:
  각 샤드: ~62.5 TPS (±5%)
  불균형 비율: ~1.1:1

솔팅 (256 버킷):
  각 샤드: ~62.5 TPS (±2%)
  불균형 비율: ~1.04:1
```

---

## 8. 벤치마크 전략

### 8.1 측정 지표 (Metrics)

```
필수 측정 지표:

1. 처리량 (Throughput)
   - ops/sec: 초당 ID 생성 수
   - 측정 구간: 1분 평균, 최대, 최소

2. 지연시간 (Latency)
   - P50, P90, P95, P99, P999, P9999, Max
   - HDR Histogram으로 정밀 측정

3. 리소스 사용량
   - CPU 사용률 (user + system)
   - 메모리 (RSS, heap, off-heap)
   - 네트워크 I/O (bytes in/out, connection count)
   - 디스크 I/O (IOPS, throughput) — DB 기반인 경우

4. 에러율
   - 중복 ID 발생 수
   - 타임아웃 수
   - 시퀀스 오버플로우 수

5. 확장성 지표
   - 노드 수 vs 처리량 (선형성 검증)
   - 동시 스레드 수 vs 처리량 (lock contention 측정)
```

### 8.2 벤치마크 환경 설정

```
하드웨어 기준:
  - CPU: 8-core (AMD EPYC / Intel Xeon) — 클라우드에서 c6i.2xlarge 급
  - RAM: 32 GB
  - 네트워크: 10 Gbps (같은 AZ 내)
  - 디스크: NVMe SSD (GP3, 3000 IOPS 이상)

소프트웨어 기준:
  - OS: Linux 5.15+ (kernel hrtimer 정밀도)
  - JVM: OpenJDK 21+ (ZGC 또는 Shenandoah) — Java 기반 시
  - Redis: 7.x (io-threads 활성화)
  - DB: PostgreSQL 16 / MySQL 8.0+

워밍업:
  - JIT 컴파일 안정화: 최소 30초 워밍업
  - 버퍼 풀 워밍업: DB 테스트 시 최소 5분
  - 측정 제외: 처음 1분 데이터 버림
```

### 8.3 부하 테스트 시나리오

```
시나리오 1: 단일 노드 최대 처리량
  목적: 단일 노드의 이론적 상한 검증
  방법: 스레드 수 1→2→4→8→16→32→64 증가
  측정: 각 단계의 처리량 + P99 레이턴시
  기대: 특정 스레드 수 이후 처리량 포화, 레이턴시 급증

시나리오 2: 일정 부하 지속 테스트
  목적: 안정 상태의 성능 특성 파악
  방법: 목표 처리량의 70%로 30분 지속
  측정: 레이턴시 분포, GC 영향, 리소스 사용 추이
  기대: 안정적 P99 유지, GC 스파이크 확인

시나리오 3: 버스트 부하 테스트
  목적: 급격한 트래픽 증가 대응 능력
  방법: 정상 부하 → 10배 스파이크(30초) → 정상 복귀
  측정: 스파이크 중/후 레이턴시, 에러율, 시퀀스 오버플로우
  기대: Snowflake 시퀀스 한도 도달 여부 확인

시나리오 4: 클러스터 확장 테스트
  목적: 수평 확장의 선형성 검증
  방법: 노드 1→2→4→8 단계적 추가
  측정: 클러스터 총 처리량, 노드 간 편차
  기대: 선형에 가까운 확장 (효율 >85%)

시나리오 5: 장애 복구 테스트
  목적: 노드/서비스 장애 시 성능 영향
  방법: 정상 운영 중 노드 1개 강제 종료
  측정: 장애 감지 시간, 재할당 시간, 중복 ID 여부
  기대: 장애 감지 < 5초, ID 유일성 보장

시나리오 6: DB 인덱스 성능 열화 테스트 (장기)
  목적: 데이터 증가에 따른 INSERT 성능 변화
  방법: 1억 행까지 순차 삽입 vs 랜덤 삽입
  측정: 매 1000만 행마다 INSERT TPS, 인덱스 크기
  기대: 랜덤 키의 성능 열화 곡선 확인
```

### 8.4 벤치마크 도구 권장

```
마이크로 벤치마크 (생성 자체):
  - JMH (Java Microbenchmark Harness) — Java
  - Go testing.B — Go
  - criterion — Rust
  - BenchmarkDotNet — .NET

부하 테스트 (서비스):
  - wrk2: 일정 처리량 유지, 레이턴시 히스토그램
  - k6: 시나리오 기반, JavaScript 스크립팅
  - Gatling: 복잡한 시나리오, HTML 보고서

모니터링:
  - Prometheus + Grafana: 실시간 메트릭
  - async-profiler: CPU/Lock 프로파일링 (Java)
  - perf: Linux 커널 레벨 프로파일링

레이턴시 측정:
  - HdrHistogram: 정밀 퍼센타일 측정
  - 주의: Coordinated Omission 방지 (wrk2 사용 이유)
```

### 8.5 벤치마크 안티패턴 주의

```
❌ 평균만 측정
  → 평균 1ms여도 P99가 100ms일 수 있음
  → 반드시 퍼센타일 분포로 측정

❌ Coordinated Omission
  → 느린 응답 시 다음 요청도 지연 → 실제 지연 과소 측정
  → wrk2의 --rate 옵션으로 일정 속도 요청 유지

❌ 짧은 테스트 시간
  → GC, 캐시 워밍업, OS 버퍼 등의 영향 미반영
  → 최소 5분, 안정성 테스트는 30분+

❌ 단일 스레드 벤치마크로 멀티스레드 성능 추정
  → lock contention은 스레드 수에 비선형 증가
  → 반드시 다양한 동시성 수준에서 측정

❌ 이상적 네트워크 조건만 테스트
  → tc (traffic control)로 지연/패킷 손실 주입
  → 실제 환경의 P99는 이상 환경 P99의 2~5배
```

---

## 9. 종합 권장사항

### 9.1 시나리오별 최적 전략

| 시나리오 | 권장 방식 | 예상 처리량 | 핵심 이유 |
|----------|----------|-----------|----------|
| **초고성능, 단일 DC** | Snowflake (내장) | ~400만/sec/node | 로컬 생성, 최소 지연 |
| **멀티 DC, 정렬 필요** | UUID v7 / ULID | ~500만/sec/node | 조정 불필요, 시간 정렬 |
| **멀티 DC, 정렬 불필요** | UUID v4 | ~800만/sec/node | 가장 단순, 최고 처리량 |
| **순차 번호 필요** | Leaf-Segment | ~10만/sec/node | DB 기반 배치 사전할당 |
| **기존 Redis 인프라** | Redis + 배치 | ~50만/sec/node | 인프라 재활용 |
| **글로벌 순서 보장** | etcd/ZK + 배치 | ~3만/sec/node | 합의 기반 순서 보장 |

### 9.2 성능 최적화 체크리스트

```
□ ID 생성은 가능하면 로컬에서 (네트워크 호출 제거)
□ 네트워크 필요 시 반드시 배치/사전할당 사용 (step ≥ 1000)
□ 이중 버퍼(Double Buffer)로 할당 지연 숨기기
□ Lock-free 구현 (CAS 기반 시퀀스)
□ UUID는 문자열이 아닌 바이너리(16 bytes)로 저장
□ 순차 ID 사용 시 DB 인덱스 이점 극대화
□ 랜덤 ID 불가피 시 UUID v7로 마이그레이션 검토
□ 핫스팟 위험 시 shard-friendly ID 설계
□ 벤치마크: HDR Histogram + wrk2로 정밀 측정
□ P999까지 SLA 정의 및 모니터링
```

### 9.3 성능 SLA 가이드라인

```
Tier 1 (핵심 트랜잭션 경로):
  처리량: > 1,000,000 IDs/sec/node
  P50: < 1 μs
  P99: < 10 μs
  P999: < 100 μs
  → 권장: Snowflake 또는 UUID v7 (내장)

Tier 2 (일반 서비스):
  처리량: > 100,000 IDs/sec/node
  P50: < 1 ms
  P99: < 5 ms
  P999: < 50 ms
  → 권장: Leaf-Segment 또는 Redis 배치

Tier 3 (관리/배치 작업):
  처리량: > 10,000 IDs/sec/node
  P50: < 5 ms
  P99: < 50 ms
  P999: < 500 ms
  → 권장: DB Sequence, etcd 기반
```

---

*본 보고서의 수치는 일반적인 서버 환경(8-core, 32GB RAM, NVMe SSD, 10Gbps NIC)에서의 참고값이며, 실제 환경에서의 벤치마크를 통해 검증이 필요합니다.*
